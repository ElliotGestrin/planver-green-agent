# PlanVer
PlanVer is a green agent designed for the [AgentBeats competition](https://agentbeats.dev) designed to evaluate how well LLM-based agents can plan and execute complex tasks. It leverages PDDL (Planning Domain Definition Language) to verify plans.

LLMs ability to reason has long been debated and *task planning* is a key benchmark for evaluating reasoning capabilities. The field of task planning focuses on solving fully defined complex tasks, in comparison to open-ended tasks like creative writing or softer planning tasks such as travel scheduling. Within the planning community, [some research](https://arxiv.org/pdf/2409.13373) arguing that they cannot reason, while [other research](https://arxiv.org/abs/2511.09378) suggests they can perform on par with traditional planners. (Though note that the above papers are from different times and use different evaluation methods.)

Within task planning, PDDL is a widely used language for defining planning problems and domains. It provides a formal way to describe the initial state, goals, and actions available to an agent. By using PDDL, PlanVer can leverage existing planning tools to verify the correctness of plans generated by LLMs.

Inspired by [AutoPlanBench](https://arxiv.org/abs/2311.09830), PlanVer translates the PDDL task definitions into natural language prompts for LLMs to generate plans. It then uses a [VAL](https://github.com/KCL-Planning/VAL) to verify the validity of these plans against the original PDDL definitions. This approach allows PlanVer to combine the strengths of LLMs in natural language understanding with the rigor of formal planning verification. Translations are done statically ahead of time using an LLM (gpt-5-nano).

PlanVer currently offers 30 domains with 30 problems each, covering a wide range of planning scenarios. The tasks are taken from the [Autoscale](https://ojs.aaai.org/index.php/ICAPS/article/view/15983) [benchmarks](https://github.com/AI-Planning/autoscale-benchmarks), keeping all of those which do not require fluents. This benchmark suite is explicitly designed to test the scalability of planners, with problems going from very easy to very hard, making it a good fit for evaluating LLM-based planning agents. 

Each domain then has the problems sorted into three difficulty levels (easy, medium, hard) based on the number of objects in the problem definition. This allows for a more granular evaluation of the agent's planning capabilities across different levels of complexity. Each level contains 10 problems.

## Adding New Domains
To add a new domain to PlanVer, follow these steps:
- Create a new directory under `domains/` with the name of the domain (e.g., `mydomain`).
- Add the PDDL domain and 30 problem files in this directory. 
- Run the description generation script to create natural language descriptions of the domain and problems:
  ```bash
  uv run generate_all_descriptions.py
  ```

Once this is done, create a pull request and the new domain will be added to the AgentBeats published green agent. 
  
## Contact & Questions
If you've got any questions, don't hesitate to write us at `elliot.gestrin@liu.se`. 
